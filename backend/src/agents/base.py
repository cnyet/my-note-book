"""
Base Agent Module - AI Life Assistant v2.0
Implements an abstract base class for all secretary agents with standardized 
lifecycle: collect -> process -> save -> sync.
Now includes Memory Retrieval capabilities.
"""
import os
import sys
import logging
from abc import ABC, abstractmethod
from datetime import datetime
from typing import Optional, Dict, Any, List

# Ensure parent directory is in path for imports
sys.path.append(os.path.join(os.path.dirname(__file__), ".."))

from integrations.llm.llm_client_v2 import create_llm_client, LLMClient
from utils.file_manager import FileManager
from core.config_loader import ConfigLoader
from core.data_synchronizer import DataSynchronizer
from core.vector_memory import VectorMemory

logger = logging.getLogger(__name__)

class BaseAgent(ABC):
    """
    Abstract Base Class for all agents.
    Provides standardized initialization, logging, and execution flow.
    """
    
    def __init__(
        self,
        name: str,
        config_path: str = "backend/config/config.ini",
        config_dict: Optional[Dict[str, Any]] = None,
    ):
        self.name = name
        self.config_path = config_path
        
        # 1. Initialize Configuration
        if config_dict:
            self.config_dict = config_dict
        else:
            config_loader = ConfigLoader(config_path)
            # Standard sections to load
            sections = ["llm", "data", "user", "system", self.name.lower()]
            self.config_dict = {}
            for section in sections:
                if config_loader.has_section(section):
                    self.config_dict[section] = config_loader.get_section(section)
                else:
                    self.config_dict[section] = {}

        # 2. Initialize Core Services
        self.llm: LLMClient = create_llm_client(config_path=config_path)
        self.file_manager = FileManager(self.config_dict.get("data", {}))
        self.synchronizer = DataSynchronizer()
        self.memory = VectorMemory()
        
        logger.info(f"Initialized {self.name} Agent")

    @abstractmethod
    def _collect_data(self, **kwargs) -> Any:
        """Step 1: Gather raw data from sources (RSS, API, Files, etc.)"""
        pass

    @abstractmethod
    def _process_with_llm(self, raw_data: Any, historical_context: str = "", **kwargs) -> str:
        """Step 2: Transform raw data into structured insight using LLM"""
        pass

    def _save_log(self, content: str, title: Optional[str] = None) -> bool:
        """Step 3: Save results to the filesystem and sync to DB"""
        if not content:
            logger.warning(f"No content to save for {self.name}")
            return False
            
        display_title = title or f"{self.name} Report"
        timestamp = datetime.now().strftime("%Y年%m月%d日 %H:%M")
        
        final_content = (
            f"# {display_title} - {timestamp}\n\n"
            f"{content}\n\n"
            f"---\n*Generated by AI Life Assistant v2.0*"
        )

        success = self.file_manager.save_daily_file(self.name.lower(), final_content)
        if success:
            logger.info(f"Successfully saved {self.name} log.")
            # Standardize sync after save
            self.synchronizer.sync_log_to_db(self.name.lower(), content)
        else:
            logger.error(f"Failed to save {self.name} log.")
        return success

    def execute(self, save_to_file: bool = True, use_memory: bool = True, **kwargs) -> str:
        """
        The standardized execution pipeline for all agents.
        """
        logger.info(f"--- {self.name} Agent: Starting Execution ---")
        
        try:
            # 1. Collection
            raw_data = self._collect_data(**kwargs)
            if not raw_data:
                logger.warning(f"No data collected by {self.name}")
                return ""

            # 2. Memory Retrieval (Optional)
            historical_context = ""
            if use_memory:
                query = str(raw_data)[:500] # Use first 500 chars as query
                historical_context = self.memory.get_context_for_llm(query, agent_type=self.name.lower())

            # 3. Processing (Now with historical context)
            result = self._process_with_llm(raw_data, historical_context=historical_context, **kwargs)
            if not result:
                logger.error(f"LLM failed to generate result for {self.name}")
                return ""

            # 4. Persistence & Synchronization
            if save_to_file:
                self._save_log(result)
                
            logger.info(f"--- {self.name} Agent: Execution Completed ---")
            return result
            
        except Exception as e:
            logger.error(f"Critical error in {self.name} execution: {str(e)}", exc_info=True)
            return f"Error: {str(e)}"

    def run(self, **kwargs) -> str:
        """Alias for execute to maintain backward compatibility"""
        return self.execute(**kwargs)
