#!/usr/bin/env python3
"""
Outfit Secretary Agent - v2.0
Responsible for daily outfit recommendations with weather integration.
Refactored to inherit from BaseAgent with Memory and standardized lifecycle.
"""

import logging
from typing import Dict, Any, Optional
from agents.base import BaseAgent
from integrations.weather.weather_client import WeatherClient

logger = logging.getLogger(__name__)

class OutfitAgent(BaseAgent):
    """AI-powered outfit recommendation agent with weather integration."""

    def __init__(self, **kwargs):
        super().__init__(name="Outfit", **kwargs)
        self.weather_client = WeatherClient()

        # Default preferences (will be enhanced by Memory in execute)
        self.user_preferences = {
            "style": "business_casual",
            "preferred_colors": ["blue", "gray", "black", "white"],
            "avoid_colors": ["yellow"],
            "special_notes": "prefer comfortable shoes for daily commute",
            "climate_preference": "slightly_warm",
        }

    def _collect_data(self, **kwargs) -> Dict[str, Any]:
        """Step 1: Gather weather data."""
        logger.info("ğŸŒ¤ï¸ Fetching weather data for outfit recommendation...")
        weather = self.weather_client.get_weather()
        
        if not weather or not weather.get("success"):
            logger.warning("Failed to fetch real weather data, using fallback.")
            # Fallback is handled by weather_client._get_mock_weather internally or we can provide here
            return weather or {"success": False}
            
        return weather

    def _process_with_llm(self, raw_data: Any, historical_context: str = "", **kwargs) -> str:
        """Step 2: Transform weather and preferences into outfit recommendation."""
        logger.info("ğŸ¤– Generating personalized outfit recommendation using LLM...")
        
        weather_summary = self.weather_client.get_weather_summary()
        weather_analysis = self.weather_client.analyze_for_outfit(raw_data)
        
        # Merge default preferences with any historical context if available
        # In v2.0, historical_context comes from VectorMemory via BaseAgent.execute()
        
        prompt_content = self._prepare_prompt(weather_summary, weather_analysis, historical_context, **kwargs)
        
        messages = [
            {
                "role": "system",
                "content": """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç€è£…é¡¾é—®ï¼Œä¸ºç”Ÿæ´»åœ¨ä¸Šæµ·çš„37å²æŠ€æœ¯ä¸“å®¶å¤§æ´ªæä¾›æ¯æ—¥ç©¿æ­å»ºè®®ã€‚
ä½ çš„èŒè´£ï¼š
1. åŸºäºå¤©æ°”æ¡ä»¶æä¾›å®ç”¨çš„ç€è£…å»ºè®®
2. è€ƒè™‘ç”¨æˆ·çš„ä¸ªäººåå¥½å’Œç”Ÿæ´»æ–¹å¼ï¼ˆå‚è€ƒå†å²è®°å¿†ï¼‰
3. æä¾›å…·ä½“ã€å¯æ‰§è¡Œçš„ç©¿æ­æ–¹æ¡ˆ
4. ç»™å‡ºå¤‡é€‰æ–¹æ¡ˆå’Œæ­é…æŠ€å·§

è¯·å§‹ç»ˆä»¥å®ç”¨æ€§å’Œèˆ’é€‚æ€§ä¸ºä¼˜å…ˆï¼Œè¯­æ°”ä¸“ä¸šã€è´´å¿ƒä¸”åŠ¡å®ã€‚""",
            },
            {"role": "user", "content": prompt_content},
        ]

        response = self.llm.send_message(
            messages=messages, max_tokens=1500, temperature=0.7
        )

        if response and isinstance(response, str):
            return response
            
        return "âŒ æ— æ³•ç”Ÿæˆç©¿æ­å»ºè®®ï¼Œè¯·æ£€æŸ¥ LLM é…ç½®ã€‚"

    def _prepare_prompt(self, weather_summary: str, weather_analysis: str, historical_context: str, **kwargs) -> str:
        """Prepare the prompt for LLM."""
        is_formal = kwargs.get("formal_requested", False)
        
        context_str = f"""ã€ä»Šæ—¥å¤©æ°”ä¿¡æ¯ã€‘
{weather_summary}

ã€å¤©æ°”åˆ†æå»ºè®®ã€‘
{weather_analysis}

ã€ç”¨æˆ·åŸºç¡€åå¥½ã€‘
- é£æ ¼ï¼š{self.user_preferences['style']}
- åå¥½é¢œè‰²ï¼š{", ".join(self.user_preferences['preferred_colors'])}
- å¤‡æ³¨ï¼š{self.user_preferences['special_notes']}
"""
        if historical_context:
            context_str += f"\nã€å†å²è®°å¿†/åå¥½å‚è€ƒã€‘\n{historical_context}\n"

        if is_formal:
            context_str += "\nâš ï¸ é‡è¦æç¤ºï¼šç”¨æˆ·ä»Šå¤©æœ‰é‡è¦æ´»åŠ¨æˆ–ä¼šè®®ï¼Œè¯·ä¼˜å…ˆæ¨èæ­£å¼åº¦è¾ƒé«˜çš„å•†åŠ¡å¥—è£…ã€‚"

        context_str += """
è¯·æä¾›ä»¥ä¸‹æ ¼å¼çš„å»ºè®®ï¼š
# ä»Šæ—¥ç©¿æ­å»ºè®® - [æ—¥æœŸ]

## ğŸ‘” ä¸»è¦ç©¿æ­
### ä¸Šè£…
...
### ä¸‹è£…
...
### é‹å±¥
...

## ğŸ”„ å¤‡é€‰æ–¹æ¡ˆ
...

## ğŸ’ é…é¥°ä¸é€šå‹¤å»ºè®®
...

## ğŸ’¡ ç©¿æ­å°è´´å£«
...
"""
        return context_str

    def interactive_mode(self):
        """Standardized interactive mode for CLI usage."""
        print("\n" + "=" * 70)
        print("ğŸ‘” Outfit Agent - Interactive Mode")
        print("=" * 70)
        
        formal = input("\nAny formal meetings today? (y/n): ").strip().lower() == 'y'
        
        print("\nğŸ¤– Running agent pipeline...")
        result = self.execute(formal_requested=formal)
        
        print("\n" + "=" * 70)
        print(result)
        print("=" * 70)
        
        return result

if __name__ == "__main__":
    agent = OutfitAgent()
    agent.interactive_mode()
